import streamlit as st
import os
import pdfplumber
import re
from dotenv import load_dotenv
from openai import OpenAI

# --- RAG æ ¸å¿ƒåº“ ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")

if not api_key:
    st.error("âŒ æ²¡æ‰¾åˆ°å¯†é’¥ï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
    st.stop()

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

st.title("ğŸ§  ä½ çš„ç¬¬äºŒå¤§è„‘ (å­¦æœ¯è®ºæ–‡ä¸“ç”¨ç‰ˆ)")

# --- 2. æ ¸å¿ƒå‡½æ•° ---
@st.cache_resource
def get_embedding_model():
    # âš ï¸ å…³é”®å‡çº§ï¼šæ¢æˆæ”¯æŒä¸­è‹±æ–‡äº’æœçš„â€œå¤šè¯­è¨€æ¨¡å‹â€
    # ä»¥å‰é‚£ä¸ªåªèƒ½æœè‹±æ–‡ï¼Œè¿™ä¸ªæ”¯æŒ 50+ ç§è¯­è¨€
    return HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

def clean_text(text):
    # ğŸ§¹ æ¸…æ´—å·¥ï¼šæŠŠè¿åœ¨ä¸€èµ·çš„å•è¯å¼ºè¡Œæ‹†å¼€ (ç®€å•å¤„ç†)
    # å¹¶å»é™¤å¤šä½™çš„æ¢è¡Œç¬¦
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def process_pdf_to_vector_db(uploaded_file):
    text_content = ""
    
    # A. è¯»å–æ–‡å­— (å°è¯•æ›´ç¨³å¥çš„è¯»å–æ–¹å¼)
    with pdfplumber.open(uploaded_file) as pdf:
        for page in pdf.pages:
            # layout=True èƒ½ä¿ç•™æ–‡å­—çš„ç©ºé—´ä½ç½®ï¼Œå‡å°‘å•è¯ç²˜è¿
            page_text = page.extract_text(layout=True)
            if page_text:
                text_content += page_text + "\n"
    
    if not text_content:
        return None, "âš ï¸ PDF å†…å®¹ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯çº¯å›¾ç‰‡ï¼‰"

    # B. æ¸…æ´—æ•°æ®
    text_content = clean_text(text_content)

    # C. åˆ‡è±†è… (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""] # ä¼˜å…ˆæŒ‰å¥å·åˆ‡
    )
    docs = [Document(page_content=x) for x in text_splitter.split_text(text_content)]
    
    # D. å‘é‡åŒ– + å…¥åº“
    embeddings = get_embedding_model()
    
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=embeddings,
        persist_directory=None
    )
    
    return vector_db, f"âœ… æˆåŠŸç´¢å¼•ï¼å¤„ç†äº† {len(docs)} ä¸ªç‰‡æ®µã€‚"

# --- 3. åˆå§‹åŒ– Session ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "è®ºæ–‡ä¼ ä¸Šæ¥ï¼Œæˆ‘æ¥å¸®ä½ è¯»ã€‚"}]
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None
if "current_file_name" not in st.session_state:
    st.session_state.current_file_name = None

# --- 4. ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ“‚ æ–‡æ¡£ä¸Šä¼ ")
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF", type="pdf")
    
    if uploaded_file:
        if uploaded_file.name != st.session_state.current_file_name:
            with st.spinner("æ£€æµ‹åˆ°æ–°è®ºæ–‡ï¼Œæ­£åœ¨åˆ‡æ¢å¤§è„‘ (é¦–æ¬¡ä¸‹è½½å¤šè¯­è¨€æ¨¡å‹éœ€ 1 åˆ†é’Ÿ)..."):
                st.session_state.vector_db = None
                st.session_state.messages = [] # æ¸…ç©ºå¯¹è¯
                
                db, msg = process_pdf_to_vector_db(uploaded_file)
                if db:
                    st.session_state.vector_db = db
                    st.session_state.current_file_name = uploaded_file.name
                    st.success(msg)
                else:
                    st.error(msg)
        else:
            st.info(f"å½“å‰æ–‡æ¡£ï¼š{uploaded_file.name}")

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.session_state.current_file_name = None
        st.rerun()

# --- 5. èŠå¤©ç•Œé¢ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# æ”¾åœ¨ä¾§è¾¹æ çš„åº•éƒ¨ï¼Œä½œä¸ºé«˜çº§é€‰é¡¹
with st.sidebar:
    st.markdown("---")
    # âœ… æ”¹åŠ¨ç‚¹ï¼šé»˜è®¤å…³é—­è°ƒè¯•æ¨¡å¼ï¼Œç•Œé¢æ›´å¹²å‡€
    show_debug = st.checkbox("ğŸ› ï¸ å¼€å¯è°ƒè¯•æ¨¡å¼ (Debug Mode)")

if prompt := st.chat_input("ç”¨ä¸­æ–‡é—®æˆ‘å…³äºè®ºæ–‡çš„é—®é¢˜..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- RAG æ£€ç´¢ ---
    context_text = ""
    if st.session_state.vector_db:
        # æœç´¢é€»è¾‘ä¸å˜
        results = st.session_state.vector_db.similarity_search(prompt, k=10)
        
        seen_content = set()
        unique_results = []
        for doc in results:
            if doc.page_content not in seen_content:
                unique_results.append(doc)
                seen_content.add(doc.page_content)
            if len(unique_results) >= 4: 
                break
        
        for i, doc in enumerate(unique_results):
            context_text += f"\n[å‚è€ƒç‰‡æ®µ {i+1}]: {doc.page_content}\n"
        
        # âœ… æ”¹åŠ¨ç‚¹ï¼šè°ƒè¯•ä¿¡æ¯åªæ˜¾ç¤ºåœ¨ä¾§è¾¹æ ï¼Œä¸å¹²æ‰°ä¸»å¯¹è¯
        if show_debug:
            with st.sidebar:
                st.subheader("ğŸ” AI å‚è€ƒçš„ç‰‡æ®µ")
                st.code(context_text, language="text") # ç”¨ä»£ç å—æ˜¾ç¤ºï¼Œæ›´ç´§å‡‘
    
    if context_text:
        full_prompt = f"ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç‰‡æ®µé‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚\n\nå‚è€ƒç‰‡æ®µï¼š\n{context_text}\n\nç”¨æˆ·é—®é¢˜ï¼š{prompt}"
    else:
        full_prompt = prompt

    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                {"role": "user", "content": full_prompt}
            ],
            stream=True,
        )
        response = st.write_stream(stream)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
