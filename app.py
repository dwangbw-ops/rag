import trafilatura
from langchain_core.documents import Document
import streamlit as st
import os
import pdfplumber
import re
from dotenv import load_dotenv
from openai import OpenAI

# --- RAG æ ¸å¿ƒåº“ ---
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
api_key = os.getenv("DEEPSEEK_API_KEY")

if not api_key:
    st.error("âŒ æ²¡æ‰¾åˆ°å¯†é’¥ï¼è¯·æ£€æŸ¥ .env æ–‡ä»¶ã€‚")
    st.stop()

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

st.set_page_config(page_title="Simplify - RAG", page_icon="ğŸ§ ")
st.title("ğŸ§  ä½ çš„ç¬¬äºŒå¤§è„‘ (PDF + URL)")

# --- 2. æ ¸å¿ƒå‡½æ•° ---

def get_embedding_model():
    # å¤šè¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··æœ
    return HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

def clean_text(text):
    # æ¸…æ´—å·¥ï¼šå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# --- æ–°å¢åŠŸèƒ½: ç½‘é¡µæŠ“å– ---
def get_web_content(url):
    """æŠ“å– URL æ­£æ–‡"""
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None
        text = trafilatura.extract(downloaded)
        return text
    except Exception as e:
        return None

# --- æ–°å¢åŠŸèƒ½: å¤„ç† URL åˆ°å‘é‡åº“ ---
def process_url_to_vector_db(url):
    # 1. æŠ“å–
    text_content = get_web_content(url)
    if not text_content:
        return None, "âš ï¸ ç½‘é¡µæŠ“å–å¤±è´¥ï¼ˆå¯èƒ½æ˜¯åçˆ¬è™«æˆ–é“¾æ¥æ— æ•ˆï¼‰"
    
    # 2. æ¸…æ´—
    text_content = clean_text(text_content)
    
    # 3. åˆ‡ç‰‡
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    docs = [Document(page_content=x, metadata={"source": url}) for x in text_splitter.split_text(text_content)]
    
    # 4. å‘é‡åŒ– + å…¥åº“
    embeddings = get_embedding_model()
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=embeddings,
        persist_directory=None
    )
    return vector_db, f"âœ… æˆåŠŸç´¢å¼•ç½‘é¡µï¼å¤„ç†äº† {len(docs)} ä¸ªç‰‡æ®µã€‚"

# --- åŸæœ‰åŠŸèƒ½: å¤„ç† PDF åˆ°å‘é‡åº“ ---
def process_pdf_to_vector_db(uploaded_file):
    text_content = ""
    try:
        with pdfplumber.open(uploaded_file) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text(layout=True)
                if page_text:
                    text_content += page_text + "\n"
    except Exception as e:
        return None, f"âŒ PDF è¯»å–é”™è¯¯: {e}"
    
    if not text_content:
        return None, "âš ï¸ PDF å†…å®¹ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯çº¯å›¾ç‰‡ï¼‰"

    text_content = clean_text(text_content)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, 
        chunk_overlap=50,
        separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
    )
    docs = [Document(page_content=x, metadata={"source": uploaded_file.name}) for x in text_splitter.split_text(text_content)]
    
    embeddings = get_embedding_model()
    vector_db = Chroma.from_documents(
        documents=docs, 
        embedding=embeddings,
        persist_directory=None
    )
    
    return vector_db, f"âœ… æˆåŠŸç´¢å¼• PDFï¼å¤„ç†äº† {len(docs)} ä¸ªç‰‡æ®µã€‚"

# --- 3. åˆå§‹åŒ– Session ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "æŠŠ PDF æ‰”è¿›æ¥ï¼Œæˆ–è€…è´´ä¸ªç½‘å€ï¼Œæˆ‘æ¥è¯»ã€‚"}]
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None
if "current_source" not in st.session_state:
    st.session_state.current_source = None

# --- 4. ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ“š æ•°æ®æº")
    
    # é€‰é¡¹ A: PDF
    st.subheader("ğŸ“„ä¸Šä¼ æœ¬åœ°æ–‡æ¡£")
    uploaded_file = st.file_uploader("é€‰æ‹© PDF", type="pdf")
    
    # é€‰é¡¹ B: URL
    st.markdown("---")
    st.subheader("ğŸŒ æˆ–è€…è¾“å…¥æ–‡ç« é“¾æ¥")
    web_url = st.text_input("è¾“å…¥ URL (ä¾‹å¦‚: 36kr/Bloomberg)")
    url_btn = st.button("æŠ“å–ç½‘é¡µ")

    # é€»è¾‘è·¯ç”±
    if uploaded_file and uploaded_file.name != st.session_state.current_source:
        with st.spinner("æ­£åœ¨å¤„ç† PDF..."):
            st.session_state.vector_db = None
            st.session_state.messages = []
            db, msg = process_pdf_to_vector_db(uploaded_file)
            if db:
                st.session_state.vector_db = db
                st.session_state.current_source = uploaded_file.name
                st.success(msg)
            else:
                st.error(msg)
    
    if url_btn and web_url:
        if web_url != st.session_state.current_source:
            with st.spinner(f"æ­£åœ¨æŠ“å– {web_url}..."):
                st.session_state.vector_db = None
                st.session_state.messages = []
                db, msg = process_url_to_vector_db(web_url)
                if db:
                    st.session_state.vector_db = db
                    st.session_state.current_source = web_url
                    st.success(msg)
                else:
                    st.error(msg)

    if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ‰€æœ‰"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.session_state.current_source = None
        st.rerun()

    st.markdown("---")
    show_debug = st.checkbox("ğŸ› ï¸ å¼€å¯è°ƒè¯•æ¨¡å¼")

# --- 5. èŠå¤©ç•Œé¢ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("ç”¨ä¸­æ–‡é—®æˆ‘..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # --- RAG æ£€ç´¢ ---
    context_text = ""
    if st.session_state.vector_db:
        results = st.session_state.vector_db.similarity_search(prompt, k=10)
        
        seen_content = set()
        unique_results = []
        for doc in results:
            if doc.page_content not in seen_content:
                unique_results.append(doc)
                seen_content.add(doc.page_content)
            if len(unique_results) >= 4: 
                break
        
        for i, doc in enumerate(unique_results):
            # è·å–æ¥æº (æ˜¯ PDF åå­— è¿˜æ˜¯ URL)
            source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
            context_text += f"\n[æ¥æº: {source} | ç‰‡æ®µ {i+1}]: {doc.page_content}\n"
        
        if show_debug:
            with st.sidebar:
                st.subheader("ğŸ” AI å‚è€ƒçš„ç‰‡æ®µ")
                st.code(context_text, language="text")
    
    if context_text:
        full_prompt = f"ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç‰‡æ®µé‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚\n\nå‚è€ƒç‰‡æ®µï¼š\n{context_text}\n\nç”¨æˆ·é—®é¢˜ï¼š{prompt}"
    else:
        full_prompt = prompt

    with st.chat_message("assistant"):
        stream = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯åŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚"},
                {"role": "user", "content": full_prompt}
            ],
            stream=True,
        )
        response = st.write_stream(stream)
    
    st.session_state.messages.append({"role": "assistant", "content": response})